{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### ðŸš© *Create a free WhyLabs account to get more value out of whylogs!*<br> \n",
    ">*Did you know you can store, visualize, and monitor whylogs profiles with the [WhyLabs Observability Platform](https://whylabs.ai/whylogs-free-signup?utm_source=whylogs-Github&utm_medium=whylogs-example&utm_campaign=Pyspark_Profiling)? Sign up for a [free WhyLabs account](https://whylabs.ai/whylogs-free-signup?utm_source=whylogs-Github&utm_medium=whylogs-example&utm_campaign=Pyspark_Profiling) to leverage the power of whylogs and WhyLabs together!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Integration\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/whylabs/whylogs/blob/mainline/python/examples/integrations/Pyspark_Profiling.ipynb)\n",
    "\n",
    "\n",
    "Hi! Perhaps you're already feeling confident with our library, but you really wish there was an easy way to plug our profiling into your existing PySpark jobs. Well, glad you've made it here, because this is what we are going to cover in this example notebook ðŸ˜ƒ\n",
    "\n",
    "If you wish to have other insights on how to use whylogs, feel free to check our [other existing examples](https://github.com/whylabs/whylogs/tree/mainline/python/examples), as they might be extremely useful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the extra dependency\n",
    "\n",
    "As we want to enable users to have exactly what they need to use from whylogs, the `pyspark` integration comes as an extra dependency. In order to have it available, simply uncomment and run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==3.1.2\n",
      "  Using cached pyspark-3.1.2-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.9\n",
      "  Using cached py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9 pyspark-3.1.2\n",
      "Collecting whylogs[spark]\n",
      "  Downloading whylogs-1.1.45-py3-none-any.whl (1.9 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.9 MB 26.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.10 in /opt/conda/lib/python3.8/site-packages (from whylogs[spark]) (4.1.1)\n",
      "Collecting whylogs-sketching>=3.4.1.dev3\n",
      "  Using cached whylogs_sketching-3.4.1.dev3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (548 kB)\n",
      "Collecting whylabs-client<1,>=0.5.1\n",
      "  Using cached whylabs_client-0.5.1-py3-none-any.whl (379 kB)\n",
      "Collecting protobuf>=3.19.4\n",
      "  Downloading protobuf-4.23.3-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304 kB 51.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyarrow<13,>=8.0.0 in /opt/conda/lib/python3.8/site-packages (from whylogs[spark]) (12.0.0)\n",
      "Requirement already satisfied: pyspark<4.0.0,>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from whylogs[spark]) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.8/site-packages (from pyarrow<13,>=8.0.0->whylogs[spark]) (1.22.3)\n",
      "Requirement already satisfied: py4j==0.10.9 in /opt/conda/lib/python3.8/site-packages (from pyspark<4.0.0,>=3.0.0->whylogs[spark]) (0.10.9)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.8/site-packages (from whylabs-client<1,>=0.5.1->whylogs[spark]) (2.8.1)\n",
      "Requirement already satisfied: urllib3>=1.25.3 in /opt/conda/lib/python3.8/site-packages (from whylabs-client<1,>=0.5.1->whylogs[spark]) (1.26.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil->whylabs-client<1,>=0.5.1->whylogs[spark]) (1.16.0)\n",
      "Installing collected packages: whylogs-sketching, whylabs-client, protobuf, whylogs\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.17.3\n",
      "    Uninstalling protobuf-3.17.3:\n",
      "      Successfully uninstalled protobuf-3.17.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp 1.6.3 requires click<8,>=7.1.1, but you have click 8.1.3 which is incompatible.\n",
      "kfp 1.6.3 requires protobuf<4,>=3.13.0, but you have protobuf 4.23.3 which is incompatible.\n",
      "kfp-pipeline-spec 0.1.13 requires protobuf<4,>=3.13.0, but you have protobuf 4.23.3 which is incompatible.\u001b[0m\n",
      "Successfully installed protobuf-4.23.3 whylabs-client-0.5.1 whylogs-1.1.45 whylogs-sketching-3.4.1.dev3\n",
      "Collecting pybars3\n",
      "  Using cached pybars3-0.9.7-py3-none-any.whl\n",
      "Collecting PyMeta3>=0.5.1\n",
      "  Using cached PyMeta3-0.5.1-py3-none-any.whl\n",
      "Installing collected packages: PyMeta3, pybars3\n",
      "Successfully installed PyMeta3-0.5.1 pybars3-0.9.7\n"
     ]
    }
   ],
   "source": [
    "# Note: you may need to restart the kernel to use updated packages.\n",
    "!pip install pyspark==3.1.2\n",
    "!pip install whylogs[spark]\n",
    "!pip install pybars3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing a SparkSession\n",
    "\n",
    "Here we will initialize a SparkSession. I'm also setting the `pyarrow` execution config, because it makes our methods even more performant. \n",
    "\n",
    ">**IMPORTANT**: Make sure you have Spark 3.0+ available in your environment, as our implementation relies on it for a smoother integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('whylogs-testing').getOrCreate()\n",
    "arrow_config_key = \"spark.sql.execution.arrow.pyspark.enabled\"\n",
    "spark.conf.set(arrow_config_key, \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data\n",
    "\n",
    "For the sake of simplicity (and computational efforts, so you can run this notebook from your local machine), we will read the Wine Quality dataset, available in this URL: \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "data_url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "spark.sparkContext.addFile(data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_dataframe = spark.read.option(\"delimiter\", \";\").option(\"inferSchema\", \"true\").csv(SparkFiles.get(\"winequality-red.csv\"), header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------\n",
      " fixed acidity        | 7.4    \n",
      " volatile acidity     | 0.7    \n",
      " citric acid          | 0.0    \n",
      " residual sugar       | 1.9    \n",
      " chlorides            | 0.076  \n",
      " free sulfur dioxide  | 11.0   \n",
      " total sulfur dioxide | 34.0   \n",
      " density              | 0.9978 \n",
      " pH                   | 3.51   \n",
      " sulphates            | 0.56   \n",
      " alcohol              | 9.4    \n",
      " quality              | 5      \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_dataframe.show(n=1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fixed acidity: double (nullable = true)\n",
      " |-- volatile acidity: double (nullable = true)\n",
      " |-- citric acid: double (nullable = true)\n",
      " |-- residual sugar: double (nullable = true)\n",
      " |-- chlorides: double (nullable = true)\n",
      " |-- free sulfur dioxide: double (nullable = true)\n",
      " |-- total sulfur dioxide: double (nullable = true)\n",
      " |-- density: double (nullable = true)\n",
      " |-- pH: double (nullable = true)\n",
      " |-- sulphates: double (nullable = true)\n",
      " |-- alcohol: double (nullable = true)\n",
      " |-- quality: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling the data with whylogs\n",
    "\n",
    "Now that we have a Spark DataFrame in place, let's see how easy it is to profile our data with whylogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from whylogs.api.pyspark.experimental import collect_column_profile_views\n",
    "\n",
    "column_views_dict = collect_column_profile_views(spark_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeap. It's done. It is **that** easy.\n",
    "\n",
    "But what do we get with a `column_views_dict`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chlorides': <whylogs.core.view.column_profile_view.ColumnProfileView object at 0x7fdb94f4bdf0>, 'total sulfur dioxide': <whylogs.core.view.column_profile_view.ColumnProfileView object at 0x7fdb94f53580>, 'pH': <whylogs.core.view.column_profile_view.ColumnProfileView object at 0x7fdb94f57a60>, 'sulphates': <whylogs.core.view.column_profile_view.ColumnProfileView object at 0x7fdb94f60ac0>, 'fixed acidity': <whylogs.core.view.column_profile_view.ColumnProfileView object at 0x7fdb94f66490>, 'density': <whylogs.core.view.column_profile_view.ColumnProfileView object at 0x7fdb94f6be20>, 'free sulfur dioxide': <whylogs.core.view.column_profile_view.ColumnProfileView object at 0x7fdb94f747f0>, 'residual sugar': <whylogs.core.view.column_profile_view.ColumnProfileView object at 0x7fdb94f77df0>, 'volatile acidity': <whylogs.core.view.column_profile_view.ColumnProfileView object at 0x7fdb94effb50>, 'alcohol': <whylogs.core.view.column_profile_view.ColumnProfileView object at 0x7fdb94f07520>, 'citric acid': <whylogs.core.view.column_profile_view.ColumnProfileView object at 0x7fdb94f0adc0>, 'quality': <whylogs.core.view.column_profile_view.ColumnProfileView object at 0x7fdb94f12d60>}\n"
     ]
    }
   ],
   "source": [
    "print(column_views_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a dictionary with one `ColumnProfileView` object per column in your dataset. And we can inspect some of the metrics on each one of them, such as the counts for a given column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599, 1599)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_views_dict[\"density\"].get_metric(\"counts\").n.value, spark_dataframe.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or their `mean` value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9967466791744841"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_views_dict[\"density\"].get_metric(\"distribution\").mean.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's check how accurate whylogs did store that `mean` calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      avg(density)|\n",
      "+------------------+\n",
      "|0.9967466791744831|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "spark_dataframe.select(mean(\"density\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not the literal exact value, but it gets really close, right? That is because we are not extracting the exact information, but we are also **not sampling** the data. `whylogs` will look at **every data point** and *statistically* decide wether or not that data point is relevant to the final calculation. \n",
    "\n",
    "Is it just me or this is extremely powerful? Yes, it is.\n",
    "\n",
    "> \"Cool! But what can I do with a bunch of `ColumnProfileView`'s from my Dataset? I want to see everything together\n",
    "\n",
    "Well, you've come to the right place, because we will inspect the next method that does just that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whylogs.api.pyspark.experimental import collect_dataset_profile_view\n",
    "\n",
    "dataset_profile_view = collect_dataset_profile_view(input_df=spark_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, that easy. You now have a `DatasetProfileView`. As you might have seen from other example notebooks in our repo, you can turn this *lightweight* object into a pandas DataFrame, and visualize all the important metrics that we've profiled, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cardinality/est</th>\n",
       "      <th>cardinality/lower_1</th>\n",
       "      <th>cardinality/upper_1</th>\n",
       "      <th>counts/inf</th>\n",
       "      <th>counts/n</th>\n",
       "      <th>counts/nan</th>\n",
       "      <th>counts/null</th>\n",
       "      <th>distribution/max</th>\n",
       "      <th>distribution/mean</th>\n",
       "      <th>distribution/median</th>\n",
       "      <th>...</th>\n",
       "      <th>type</th>\n",
       "      <th>types/boolean</th>\n",
       "      <th>types/fractional</th>\n",
       "      <th>types/integral</th>\n",
       "      <th>types/object</th>\n",
       "      <th>types/string</th>\n",
       "      <th>types/tensor</th>\n",
       "      <th>frequent_items/frequent_strings</th>\n",
       "      <th>ints/max</th>\n",
       "      <th>ints/min</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alcohol</th>\n",
       "      <td>65.000010</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.003256</td>\n",
       "      <td>0</td>\n",
       "      <td>1599</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.90000</td>\n",
       "      <td>10.422983</td>\n",
       "      <td>10.20000</td>\n",
       "      <td>...</td>\n",
       "      <td>SummaryType.COLUMN</td>\n",
       "      <td>0</td>\n",
       "      <td>1599</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chlorides</th>\n",
       "      <td>153.000058</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>153.007697</td>\n",
       "      <td>0</td>\n",
       "      <td>1599</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.61100</td>\n",
       "      <td>0.087467</td>\n",
       "      <td>0.07900</td>\n",
       "      <td>...</td>\n",
       "      <td>SummaryType.COLUMN</td>\n",
       "      <td>0</td>\n",
       "      <td>1599</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>citric acid</th>\n",
       "      <td>80.000016</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>80.004010</td>\n",
       "      <td>0</td>\n",
       "      <td>1599</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.270976</td>\n",
       "      <td>0.26000</td>\n",
       "      <td>...</td>\n",
       "      <td>SummaryType.COLUMN</td>\n",
       "      <td>0</td>\n",
       "      <td>1599</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>density</th>\n",
       "      <td>439.557368</td>\n",
       "      <td>433.943761</td>\n",
       "      <td>445.310933</td>\n",
       "      <td>0</td>\n",
       "      <td>1599</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00369</td>\n",
       "      <td>0.996747</td>\n",
       "      <td>0.99675</td>\n",
       "      <td>...</td>\n",
       "      <td>SummaryType.COLUMN</td>\n",
       "      <td>0</td>\n",
       "      <td>1599</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed acidity</th>\n",
       "      <td>96.000023</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>96.004816</td>\n",
       "      <td>0</td>\n",
       "      <td>1599</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.90000</td>\n",
       "      <td>8.319637</td>\n",
       "      <td>7.90000</td>\n",
       "      <td>...</td>\n",
       "      <td>SummaryType.COLUMN</td>\n",
       "      <td>0</td>\n",
       "      <td>1599</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               cardinality/est  cardinality/lower_1  cardinality/upper_1  \\\n",
       "column                                                                     \n",
       "alcohol              65.000010            65.000000            65.003256   \n",
       "chlorides           153.000058           153.000000           153.007697   \n",
       "citric acid          80.000016            80.000000            80.004010   \n",
       "density             439.557368           433.943761           445.310933   \n",
       "fixed acidity        96.000023            96.000000            96.004816   \n",
       "\n",
       "               counts/inf  counts/n  counts/nan  counts/null  \\\n",
       "column                                                         \n",
       "alcohol                 0      1599           0            0   \n",
       "chlorides               0      1599           0            0   \n",
       "citric acid             0      1599           0            0   \n",
       "density                 0      1599           0            0   \n",
       "fixed acidity           0      1599           0            0   \n",
       "\n",
       "               distribution/max  distribution/mean  distribution/median  ...  \\\n",
       "column                                                                   ...   \n",
       "alcohol                14.90000          10.422983             10.20000  ...   \n",
       "chlorides               0.61100           0.087467              0.07900  ...   \n",
       "citric acid             1.00000           0.270976              0.26000  ...   \n",
       "density                 1.00369           0.996747              0.99675  ...   \n",
       "fixed acidity          15.90000           8.319637              7.90000  ...   \n",
       "\n",
       "                             type  types/boolean  types/fractional  \\\n",
       "column                                                               \n",
       "alcohol        SummaryType.COLUMN              0              1599   \n",
       "chlorides      SummaryType.COLUMN              0              1599   \n",
       "citric acid    SummaryType.COLUMN              0              1599   \n",
       "density        SummaryType.COLUMN              0              1599   \n",
       "fixed acidity  SummaryType.COLUMN              0              1599   \n",
       "\n",
       "               types/integral  types/object  types/string  types/tensor  \\\n",
       "column                                                                    \n",
       "alcohol                     0             0             0             0   \n",
       "chlorides                   0             0             0             0   \n",
       "citric acid                 0             0             0             0   \n",
       "density                     0             0             0             0   \n",
       "fixed acidity               0             0             0             0   \n",
       "\n",
       "               frequent_items/frequent_strings  ints/max  ints/min  \n",
       "column                                                              \n",
       "alcohol                                    NaN       NaN       NaN  \n",
       "chlorides                                  NaN       NaN       NaN  \n",
       "citric acid                                NaN       NaN       NaN  \n",
       "density                                    NaN       NaN       NaN  \n",
       "fixed acidity                              NaN       NaN       NaN  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "dataset_profile_view.to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting as a file\n",
    "\n",
    "After collecting profiles, it is a good practice to store them as a file. This will allow you to later on read them back, merge with future profiles and track how is your data behaving along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 'my_super_awesome_profile.bin')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_profile_view.write(path=\"my_super_awesome_profile.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it, you have just written a profile generated with spark to your local environment! If you wish to upload to different locations, such as s3, whylabs or others, please make sure to check out our [other examples](https://github.com/whylabs/whylogs/tree/mainline/python/examples) page.\n",
    "\n",
    "Hopefully this tutorial will help you get started to profile and observe your data behaviour in your Spark jobs with almost no friction :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important note\n",
    "\n",
    "As you might have seen from the imports, currently this pyspark implementation is the **experimental** phase. We ran some benchmark ourselves with it, and for the sake of example, a `90Gb` dataset with 80M rows could be profiled in under 3 minutes! Cool, right? But we still want more users to try this on their own, see if there are places to be improved and give us feedback before we make it officially **the** spark module here. \n",
    "Please, feel free to reach out to our [community Slack](https://communityinviter.com/apps/whylabs-community/rsqrd-ai-community) and interact with us there. We will love to hear from you :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16a10773934acde374a1cd808bcd53b1085f60e17ec18f4c0c26564dd890a5a0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
